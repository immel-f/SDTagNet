<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://immel-f.github.io/sdtagnet/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction - Fabian Immel, Jan-Hendrik
    Pauls, Richard Fehler, Frank Bieder, Jonas Merkert, Christoph Stiller</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <style>
    .carousel {
      display: flex;
      justify-content: center;
      /* Centers the items horizontally */
      align-items: center;
      /* Centers the items vertically */
    }

    .results-carousel {
      max-width: 80%;
      /* Adjust the percentage as needed */
      height: auto;
      /* Maintains the aspect ratio */
      display: block;
      /* Centers the image */
      margin: 0 auto;
      /* Centers the image */
    }

    .carousel-image-qual {
      max-width: 70%;
      /* Adjust the percentage as needed */
      height: auto;
      /* Maintains the aspect ratio */
      display: block;
      /* Centers the image */
      margin: 0 auto;
      /* Centers the image */
    }
  </style>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "Fabian Immel",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://immel-f.github.io/sdtagnet/",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://mrt.kit.edu",
    "logo": "https://mrt.kit.edu/favicon.ico",
    "sameAs": [
      "https://github.com/immel-f"
    ]
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://arxiv.org/abs/2411.10316" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>M3TR: A Generalist Model for Real-World HD Map Completion</h5>
            <!-- TODO: Replace with brief description -->
            <p>A generalist model for online HD map construction with HD map priors that can handle variable prior
              scenarios</p>
            <!-- TODO: Replace with venue and year -->
            <!-- <span class="work-venue">Conference/Journal 2024</span> -->
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i> -->
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- TODO: Replace with your paper title -->
              <h1 class="title is-1 publication-title">SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD
                Map Construction</h1>
              <h2 class="subtitle has-text-centered">
                <i>Accepted for publication at NeurIPS 2025</i>
              </h2>
              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your paper authors and their personal links -->
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=de&user=uHlmd9QAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank">Fabian Immel</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=0LbD7HUAAAAJ&hl=de&oi=ao"
                    target="_blank">Jan-Hendrik
                    Pauls</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=de&user=gOQYH4AAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank">Richard Fehler</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=mAMWuMUAAAAJ&hl=de&oi=ao" target="_blank">Frank
                    Bieder</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.de/citations?user=lv_OG7MAAAAJ&hl=de&oi=ao" target="_blank">Jonas
                    Merkert</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=OeAQ2c0AAAAJ&hl=de&oi=ao" target="_blank">Christoph
                    Stiller</a><sup>2</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>FZI Research Center for Information Technology, &nbsp;
                  <sup>2</sup>Karlsruhe Institute of Technology</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2506.08997" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- TODO: Add your supplementary material PDF or remove this section -->
                  <!-- <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/immel-f/SDTagNet" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2506.08997" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Teaser video-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <!-- TODO: Replace with your teaser video -->
          <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
            <!-- TODO: Add your video file path here -->
            <source src="static/videos/demo_combined_cut_downscaled.mp4" type="video/mp4">
          </video>
          <!-- TODO: Replace with your video description -->
          <h2 class="subtitle has-text-centered">
            Example Results of SDTagNet on the Argoverse 2 dataset in comparison with the existing navigation map
            encoding methods PMapNet and SMERF.
          </h2>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <!-- TODO: Replace with your paper abstract -->
              <p>
                Autonomous vehicles rely on detailed and accurate environmental information to operate safely. High
                definition (HD) maps offer a promising solution, but their high maintenance cost poses a significant
                barrier to scalable deployment. This challenge is addressed by online HD map construction methods, which
                generate local HD maps from live sensor data. However, these methods are inherently limited by the short
                perception range of onboard sensors. To overcome this limitation and improve general performance, recent
                approaches have explored the use of standard definition (SD) maps as prior, which are significantly
                easier
                to maintain. We propose SDTagNet, the first online HD map construction method that fully utilizes the
                information of widely available SD maps, like OpenStreetMap, to enhance far range detection accuracy.
                Our
                approach introduces two key innovations. First, in contrast to previous work, we incorporate not only
                polyline SD map data with manually selected classes, but additional semantic information in the form of
                textual annotations. In this way, we enrich SD vector map tokens with NLP-derived features, eliminating
                the dependency on predefined specifications or exhaustive class taxonomies. Second, we introduce a
                point-level SD map encoder together with orthogonal element identifiers to uniformly integrate all types
                of map elements. Experiments on Argoverse 2 and nuScenes show that this boosts map perception
                performance
                by up to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP (+20%) w.r.t.
                previous
                approaches that already use SD map priors.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!-- Image carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- TODO: Replace with your research result images -->
              <img src="static/images/architecture_overview.png" alt="First research result visualization"
                loading="lazy" />
              <!-- TODO: Replace with description of this result -->
              <h2 class="subtitle has-text-centered">
                Overview of the model architecture of SDTagNet. To fully exploit textual annotations and
                all element types in large public SD map databases like OpenStreetMap, SDTagNet introduces
                novel NLP tag embedding and SD map encoder modules. Text annotation embeddings are first
                computed with a BERT embedding model. They are then fused with scene-level context in a SD
                map encoder, which uses graph transformer-like methods to flexibly encode points, polylines and
                element relations. The encoded information is finally supplied to the base model via cross-attention.
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/input_visualization.png" alt="Second research result visualization"
                loading="lazy" />
              <h2 class="subtitle has-text-centered">
                Visualization of the SD map prior input data utilized by existing methods. Existing
                approaches are limited to rasterized images or polylines with manually defined classes. SDTagNet is
                the first method that can handle open-vocabulary textual annotations and diverse element types such
                as points, polylines, and relational information.
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/pretraining_objective.png" alt="Third research result visualization"
                loading="lazy" />
              <h2 class="subtitle has-text-centered">
                Example of the tag embedding contrastive pretraining objective. A positive sample is
                selected from tagsets with the same semantically meaningful tags, but different not meaningful ones
                (like the street name). Negative samples are selected from all other unique tagsets. The number of
                negative samples in practice is much larger than depicted here to prevent unstable training.
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/sd_map_enc_detail.png" alt="Fourth research result visualization"
                loading="lazy" />
              <h2 class="subtitle has-text-centered">
                Detailed design of the SD map encoder and its queries. Each point query is composed of
                the positional sin/cos encoding of the point, the respective tag embedding and orthogonal random
                features (ORF) that function as element identifiers and can model graph edges.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Image carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel2" class="carousel results-carousel carousel-image-qual">
            <div class="item">
              <!-- TODO: Replace with your research result images -->
              <img src="static/images/av2_table.png" alt="First research result visualization" loading="lazy" />
              <!-- TODO: Replace with description of this result -->
              <h2 class="subtitle has-text-centered">
                Comparison of SD map prior encoding methods on Argoverse 2 , with a geographical
                split. *: With the 7 classes from SMERF in the input features, which are not used in the original
                work. †: With OSM nodes in the input features, which are not used in the original work. All models
                are trained for 24 epochs.
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/av2_ablation_study.png" alt="Second research result visualization"
                loading="lazy" />
              <h2 class="subtitle has-text-centered">
                Ablation study of different encoder components on the Argoverse 2 dataset. All experiments
                are in the near range setting and all models are trained for 24 epochs. †: With OSM nodes in the input
                features, which are not used in the original work. + BEV Ft.: With BEV features as an additional
                prior mode.
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/av2_qual.png" alt="Third research result visualization" loading="lazy" />
              <h2 class="subtitle has-text-centered">
                Further qualitative comparison of SDTagNet with PMapNet (all info.) and SMERF (all
                info.) on Argoverse 2 in the far range setting. SDTagNet is able to utilize text-annotated information
                such as number of lanes and oneway roads to improve prediction results.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->


    <!-- Youtube video -->
    <!-- <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          
          <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">

              <div class="publication-video">
                
                <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0"
                  allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End youtube video -->


    <!-- Video carousel -->
    <!-- <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Another Carousel</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-video1">
              
              <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
                
                <source src="static/videos/carousel1.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-video2">
              
              <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
               
                <source src="static/videos/carousel2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-video3">
              
              <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
                
                <source src="static/videos/carousel3.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End video carousel -->






    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Poster</h2>
          <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

        </div>
      </div>
    </section> -->
    <!--End paper poster -->



    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{SDTagNet2025,
  title={SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction},
  author={ Fabian Immel and Jan-Hendrik Pauls and Richard Fehler and Frank Bieder and Jonas Merkert and Christoph Stiller},
  booktitle = {39th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025},
  url={https://immel-f.github.io/sdtagnet/}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>